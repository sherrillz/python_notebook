{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Data Manipulating with Python\n",
    "Set up some basic packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.read_csv(path/file.csv)\n",
    "new_df.to_csv(path/file.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the head of the homelessness data\n",
    "print(homelessness.head())\n",
    "\n",
    "# Print information about homelessness: shows information on each of the columns, such as the data type and number of missing values\n",
    "print(homelessness.info())\n",
    "\n",
    "# Print the shape of homelessness:  returns the number of rows and columns of the DataFrame.\n",
    "print(homelessness.shape)\n",
    "\n",
    "# Print a description of homelessness:  calculates a few summary statistics for each column.\n",
    "print(homelessness.describe())\n",
    "# Print the column index of homelessness\n",
    "print(homelessness.columns)\n",
    "\n",
    "# Print the row index of homelessness\n",
    "print(homelessness.index)\n",
    "#Multiple aggregation\n",
    "sales[[\"temperature_c\", \"fuel_price_usd_per_l\", \"unemployment\"]].agg([np.mean,np.median])\n",
    "\n",
    "#Cumulative statistics \n",
    "sales[['col1','col2']].cummax() #returns a new Series where each element is the maximum of all previous elements up to that position\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Sorting and Subsetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort column\n",
    "dp.sort_values('column', ascending = True) #By default, it's true\n",
    "#Sort multiple\n",
    "dp.sort_values(['column1','column2'],ascending =[True,False] )\n",
    "#Subsetting column 1 \n",
    "dp[['column1','column2']]\n",
    "# Subsetting column 2\n",
    "col_sub = ['column1','column2']\n",
    "dp[col_sub]\n",
    "# Subsetting rows on condition\n",
    "dp[dp[column]>2]\n",
    "#Subsetting on multiple conditions\n",
    "dp[(condition1) & (condition2) ]\n",
    "dp[dp['colour'].isin(['black','red'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Manipulating DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add a new column\n",
    "df['new_col'] = df['col'] * 100 \n",
    "## Drop Duplicates\n",
    "df.drop_duplicates(subset=['col1','col2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Grouped Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate by for all columns\n",
    "books.agg({\"rating\": [\"mean\",\"std\"],\"year\": [\"median\"]})\n",
    "\n",
    "# Group by 'Department' and calculate mean salary\n",
    "grouped_mean = df.groupby('Department')['Salary'].mean()\n",
    "\n",
    "# Group by 'Department' and calculate sum and mean salary\n",
    "grouped_stats = df.groupby('Department')['Salary'].agg(['sum', 'mean'])\n",
    "# Create 3 new columns for aggregated calculation\n",
    "borough_stats = schools.groupby('borough').agg(\n",
    "    num_schools=('school_name','size'), #  the number of schools in the borough.\n",
    "    average_SAT=('total_SAT','mean'), # the mean of column \"total_SAT\".\n",
    "    std_SAT = ('total_SAT','std')).round(2) # the standard deviation of column \"total_SAT\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Pivot Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a pivot table\n",
    "#*.pivot_table() takes the mean of each group by default\n",
    "pivot = df.pivot_table(  \n",
    "                       values='Revenue', \n",
    "                       index='Date', \n",
    "                       columns='Department', \n",
    "                       aggfunc=[sum,np.mean],\n",
    "                       fill_value = 0, # fill na with 0\n",
    "                       margin = True # summing all rows and columns\n",
    "                       )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Working with Pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mean by columns\n",
    "mean_temp_by_year = temp_by_country_city_vs_year.mean(axis = \"index\") # Get the mean by index axis = \"columns\"\n",
    "\n",
    "# Filter for the columns that had the highest mean\n",
    "mean_temp_by_year[mean_temp_by_year == mean_temp_by_year.max()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data filtering in Pandas\n",
    "Since ``df['column']`` return the result as ``Series``\n",
    "One way of filtering the data is using a boolean series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by one condition\n",
    "df['column'] = condition # Output a series of ['true','false',...]\n",
    "con = (df['column']= condition)\n",
    "# return the data with this condition\n",
    "result = df[con] \n",
    "#i.e:\n",
    "np.logical_and(brics[\"area\"] > 8, brics[\"area\"] < 10)\n",
    "brics[np.logical_and(brics[\"area\"] > 8, brics[\"area\"] < 10)]\n",
    "\n",
    "# Filter multiple conditions\n",
    "movie_short = netflix_df\n",
    "[\n",
    "    (netflix_df['type'] == 'Movie')\n",
    "    &(netflix_df['genre'] == 'Action')\n",
    "    & (netflix_df['release_year'] >= 1990) \n",
    "    & (netflix_df['release_year'] < 2000)\n",
    "    &(netflix_df['duration'] <=90)\n",
    "]\n",
    "\n",
    "# Filter sting columns:\n",
    "\n",
    "# Filter the rows that contains '@acacia.com'\n",
    "print(users[users['email'].str.contains('@acacia.com')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slicing and Indexing DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Explicit indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set index\n",
    "dogs_ind = dogs.set_index(\"name\")\n",
    "#Remove index\n",
    "dogs_ind = dogs.reset_index()\n",
    "#drop index\n",
    "dogs_ind = dogs.reset_index(drop = True)\n",
    "#Subsetting through index\n",
    "dogs_ind.loc[\"ind1\",\"ind2\"]\n",
    "#set pairs as index\n",
    "# List of tuples: Brazil, Rio De Janeiro & Pakistan, Lahore\n",
    "rows_to_keep = [('Brazil','Rio De Janeiro'),('Pakistan','Lahore')]\n",
    "# Subset for rows to keep\n",
    "print(temperatures_ind.loc[rows_to_keep])\n",
    "#Sorting index\n",
    "dogs_ind3.sort_index(level=[\"color\",\"breed\"], ascending=[True, False])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Slicing and Subsetting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Using ``Loc``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing the outer index level\n",
    "dogs_srt.loc[\"Chow Chow\":\"Poodle\"]\n",
    "# Slicing the inner index levels\n",
    "dogs_srt.loc[(\"Labrador\",\"Brown\"):(\"Schnauzer\",\"Grey\")]\n",
    "# Slicing columns\n",
    "temperatures_srt.loc[:,'date':'avg_temp_c']\n",
    "# Slicing rows and columns:\n",
    "temperatures_srt.loc[('India','Hyderabad'):('Iraq','Baghdad'),'date':'avg_temp_c']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Using ``iloc``   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 23rd row, 2nd column (index 22, 1)\n",
    "print(temperatures.iloc[22,1])\n",
    "\n",
    "# Use slicing to get the first 5 rows\n",
    "print(temperatures.iloc[:5])\n",
    "\n",
    "# Use slicing to get columns 3 to 4\n",
    "print(temperatures.iloc[:,2:4])\n",
    "\n",
    "# Use slicing in both directions at once\n",
    "print(temperatures.iloc[:5,2:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization from DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_pack[\"column1\"].hist(bins=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Bar plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a group by\n",
    "avg_weight_by_breed = dog_pack.groupby(\"breed\")[\"weight_kg\"].mean()\n",
    "# Create bar\n",
    "avg_weight_by_breed.plot(kind=\"bar\"title=\"Mean Weight by Dog Breed\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Line plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create line\n",
    "sulLy.plot(x=\"date\"y=\"weight_kg\",kind=\"line\")\n",
    "plt.show()\n",
    "# Rotating axis labels\n",
    "sully.plot(x=\"date\",y=\"weight_kg\",kind=\"line\", rot=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Scatter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create line Scatter\n",
    "dog_pack.plot(x=\"height_cm\",y=\"weight_kg\", kind=\"scatter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Histgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_pack[dog_pack[\"sex\"]==\"F\"][\"height_cm\"].hist(alpha=0.7,bins = 20) # alpha is the transparency\n",
    "dog_pack[dog_pack[\"sex\"]==\"M\"][\"height_cm\"].hist(alpha=0.7,bins = 20)\n",
    "plt.legend([\"F\",\"M\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Detect Missing Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Detecting Missing Value\n",
    "df.isna() # Return each columns and rows\n",
    "#Detecting any Missing value\n",
    "df.isna().any() # Return T/F of the summary of column\n",
    "# Counting missing value\n",
    "df.isna().sum()\n",
    "# Plotting missing values\n",
    "df.isna().sum().plot(kind = 'bar')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dealing Missing value\n",
    "- Drop missing values\n",
    "    5% or less of total values\n",
    "- Impute mean, median, mode\n",
    "  Depends on distribution and context\n",
    "- Impute by sub-group \n",
    " Different experience levels have different median salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values\n",
    "df.isna().sum()\n",
    "\n",
    "# If the NA is only a small amount and doesn't really matter\n",
    "df.dropna()\n",
    "# Replace with 0\n",
    "df.fillna(0)\n",
    "\n",
    "# Find the columns that have the missing value <=0.5 total value\n",
    "threshold = len(salaries) * 0.05\n",
    "cols_to_drop = df.columns[salaries.isna().sum() <= threshold]\n",
    "# Drop the NA in these columns\n",
    "salaries.dropna(subset=cols_to_drop, inplace=True)\n",
    "\n",
    "# Impute a summary statistics\n",
    "cols_with_missing_values = df.columns[df.isna().sum()> 0]\n",
    "for col in cols_with_missing_values[:-1]:\n",
    "    df[col].fillna(df[col].mode()[0])\n",
    "\n",
    "# Impute by sub-group\n",
    "salaries_dict = salaries.groupby(\"Experience\")[\"Salary_UsD\"].median().to_dict()\n",
    "# Output: {'Entry':!55380.0，'Executive':135439.0，'Mid':74173.5，'Senior':128903.0}\n",
    "salaries[\"Salary UsD\"] = salaries[\"Salary usp\"].fillna(salaries[\"Experience\"].map(salaries_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create rows by rows through a list of dictionary\n",
    "list_of_dicts =[\n",
    "    {\"name\":\"Ginger\",\n",
    "    \"breed\":\"Dachshund\",\n",
    "    \"height_cm\": 22,\n",
    "    \"weight_kg\":10,\n",
    "    \"date_of_birth\":\"2019-03-14\"}]\n",
    "new_dogs = pd.DataFrame(list_of_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create colum by column through a dictionary of list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_lists = {\n",
    "    \"name\":[\"Ginger\",\"Scout\"],\n",
    "    \"breed\":[\"Dachshund\",\"Dalmatian\"],\n",
    "    \"height_cm\":[22,59],\n",
    "    \"weight_kg\":[10,25],\n",
    "    \"date_of_birth\":[\"2019-03-14\",\"2019-05-09\"]}\n",
    "new_dogs2 = pd.DataFrame(dict_of_lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
